<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws on musings about silly things</title>
    <link>http://localhost:1313/categories/aws/</link>
    <description>Recent content in Aws on musings about silly things</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>hello@heiber.im (Moritz Heiber)</managingEditor>
    <webMaster>hello@heiber.im (Moritz Heiber)</webMaster>
    <lastBuildDate>Sun, 21 Feb 2016 16:21:11 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/aws/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Building a Kinesis-enabled heka package for logging on AWS</title>
      <link>http://localhost:1313/post/building-docker-heka/</link>
      <pubDate>Sun, 21 Feb 2016 16:21:11 +0100</pubDate>
      <author>hello@heiber.im (Moritz Heiber)</author>
      <guid>http://localhost:1313/post/building-docker-heka/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/mozilla-services/heka&#34;&gt;Heka&lt;/a&gt; is an excellent logging and processing framework made by none other than &lt;a href=&#34;https://mozilla.org&#34;&gt;Mozilla&lt;/a&gt;, the creators of &lt;a href=&#34;https://getfirefox.com&#34;&gt;Firefox&lt;/a&gt;. There are other frameworks like &lt;a href=&#34;https://www.elastic.co/products/logstash&#34;&gt;logstash&lt;/a&gt;, &lt;a href=&#34;https://fluentd.org&#34;&gt;fluentd&lt;/a&gt;, or proprietary solutions like &lt;a href=&#34;https://loggly.com&#34;&gt;Loggly&lt;/a&gt; or &lt;a href=&#34;https://www.splunk.com&#34;&gt;Splunk&lt;/a&gt;. But none of them combine the flexibility of heka, through its included &lt;a href=&#34;https://www.lua.org&#34;&gt;Lua&lt;/a&gt; scripting engine, and the scalability, through its modular engine based on &lt;a href=&#34;https://golang.org&#34;&gt;Golang&lt;/a&gt;, with the extensibility through its &lt;a href=&#34;http://hekad.readthedocs.org/en/latest/developing/plugin.html&#34;&gt;plugin ecosystem&lt;/a&gt; and versatile processing workflow engine.&lt;/p&gt;

&lt;p&gt;If you were starting fresh I would probably recommend fluentd. It&amp;rsquo;s easy to get started with (it literally takes a couple of seconds to write an appropriate &lt;code&gt;Gemfile&lt;/code&gt;), it has a powerful processing pipeline of its own and a vast plugin ecosystem. It even comes with built-in high availability. It lets you take on your task with data driven discussions quickly and easily.&lt;/p&gt;

&lt;p&gt;Unfortunately its very core, Ruby, is also its biggest disadvantage. When you&amp;rsquo;re talking about a couple of 100 of megabytes a day, fluentd will get you there no problems. But once you&amp;rsquo;re hitting a couple of 100 of gigabytes of logging and metric data a day it&amp;rsquo;s going to falter sooner or later, especially if you&amp;rsquo;re doing pre- and post-processing in the same pipeline.&lt;/p&gt;

&lt;p&gt;This is where heka comes in.&lt;/p&gt;

&lt;p&gt;Heka is written in Golang, making it a very small, portable package. With the available plugins, and its extensibility for processing and filtering offered by the internal Lua parser engine, it&amp;rsquo;s perfectly capable of scaling to pretty much any workload you throw at it.&lt;/p&gt;

&lt;h2 id=&#34;streaming-logs-with-heka-and-kinesis:06f5b5f8600296dbbb62f5839e459228&#34;&gt;Streaming logs with heka and Kinesis&lt;/h2&gt;

&lt;p&gt;A traditional setup with a logging daemon running somewhere would mean aggregating logs, either by tailing local files, or accepting messages on a socket or port in a structured format such as JSON or messagepack. And that&amp;rsquo;s all right.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been working with a lot of projects deployed to AWS lately. Decoupling the log streaming from the transport, the processing and the storage obviously makes sense. For streams you can chose either SQS or Kinesis on AWS to use readily available services. I usually go for Kinesis over SQS for &lt;a href=&#34;http://aws.amazon.com/kinesis/streams/faqs/&#34;&gt;many reasons&lt;/a&gt;, but mostly it&amp;rsquo;s the streaming part of what Kinesis has tooffer and its seamless Lambda integration that seals the deal for me. Also, since you&amp;rsquo;re essentially just paying for a stream of data and not per-record events, Kinesis tends to be a lot cheaper than SQS for large volume data transfers.&lt;/p&gt;

&lt;p&gt;Also, with the addition of &lt;a href=&#34;https://aws.amazon.com/blogs/aws/amazon-kinesis-firehose-simple-highly-scalable-data-ingestion/&#34;&gt;Kinesis Firehose&lt;/a&gt; lately, processing huge amounts of already aggregated data has become even easier with Kinesis. Try doing that with SQS. Your billing department will not like it.&lt;/p&gt;

&lt;h3 id=&#34;kinesis-as-an-output-for-heka:06f5b5f8600296dbbb62f5839e459228&#34;&gt;Kinesis as an output for heka&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://hekad.readthedocs.org/en/latest/config/outputs/index.html&#34;&gt;There is a sizable list of available output plugins&lt;/a&gt; for heka. Unfortunately, Kinesis isn&amp;rsquo;t one of the natively developed plugins. You will have to build it yourself.&lt;/p&gt;

&lt;p&gt;Fear not though, &lt;a href=&#34;https://github.com/crewton/heka-plugins&#34;&gt;there is a solution provided by the community&lt;/a&gt;. When you are trying to use it you will probably want to use the &lt;a href=&#34;https://github.com/MattLTW/heka-plugins&#34;&gt;slightly patched-up version&lt;/a&gt; of my colleague &lt;a href=&#34;https://github.com/MattLTW&#34;&gt;Matthew Lloyd&lt;/a&gt;. It has support for the latest version of heka, 0.10.0 (and beyond).&lt;/p&gt;

&lt;h2 id=&#34;building-your-own-heka-package-with-docker:06f5b5f8600296dbbb62f5839e459228&#34;&gt;Building your own heka package with Docker&lt;/h2&gt;

&lt;p&gt;Docker seems like the ideal way of building a system package. It&amp;rsquo;s isolated from the overall operating system you&amp;rsquo;re building it, your build environment can be set up in an instant, over and over again, and there are no remnants left behind afterwards.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m going to use the following &lt;code&gt;Dockerfile&lt;/code&gt; for building a heka package with the Kinesis plugin enabled:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Dockerfile&#34;&gt;FROM golang:latest
MAINTAINER Moritz Heiber &amp;lt;hello@heiber.im&amp;gt;

ARG uid

RUN apt-get update &amp;amp;&amp;amp; \
    apt-get install -y curl cmake git mercurial build-essential debhelper rpm

RUN useradd --uid ${uid} -mU heka

# 0.10.0
ENV version v0.10.0

RUN git clone -n https://github.com/mozilla-services/heka /tmp/heka
WORKDIR /tmp/heka
RUN git checkout ${version}

# Add the kinesis plugin
RUN sed -i &#39;152igit_clone\(https\:\/\/github.com\/vaughan0\/go-ini\ a98ad7ee00ec53921f08832bc06ecf7fd600e6a1\)\ngit_clone\(https\:\/\/github.com\/aws\/aws-sdk-go\ 90a21481e4509c85ee68b908c72fe4b024311447\)\nadd_dependencies\(aws-sdk-go\ go-ini\)&#39; cmake/externals.cmake
RUN echo &amp;quot;add_external_plugin(git https://github.com/MattLTW/heka-plugins.git master kinesis)&amp;quot; &amp;gt;&amp;gt; cmake/plugin_loader.cmake

ADD build-heka.sh .
RUN chown -R heka:heka /tmp/heka

VOLUME /tmp/heka-build

USER heka
CMD [&amp;quot;./build-heka.sh&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;step-by-step:06f5b5f8600296dbbb62f5839e459228&#34;&gt;Step by step&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-Dockerfile&#34;&gt;ARG uid
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a build argument, only introduced to Dockerfiles recently. You specify it when you run &lt;code&gt;docker build&lt;/code&gt;. In this instance it helps us provide an image which belongs to the user running the package build. More on this later.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Dockerfile&#34;&gt;RUN apt-get update &amp;amp;&amp;amp; \
    apt-get install -y curl cmake git mercurial build-essential debhelper rpm
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These are all the required packages for building heka. Since the &lt;code&gt;golang:latest&lt;/code&gt; &lt;code&gt;Dockerfile&lt;/code&gt; is based on Debian Jessie we have the &lt;code&gt;apt-get&lt;/code&gt; package manager at our disposal. Heka is using &lt;code&gt;git&lt;/code&gt; and &lt;code&gt;mercurial&lt;/code&gt; internally to manage dependencies.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-Dockerfile&#34;&gt;RUN useradd --uid ${uid} -mU go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This adds a user with the same system id of the user running the Docker build. This is important because otherwise the package coming out of the container is going to belong to the user root, which you might be able to copy somewhere, but especially coming out of a pipeline it&amp;rsquo;s going be a tough problem to deal with.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamics DNS with pfSense and Route53</title>
      <link>http://localhost:1313/post/dynamics-dns-with-pfsense-and-route53/</link>
      <pubDate>Fri, 28 Feb 2014 12:17:36 +0100</pubDate>
      <author>hello@heiber.im (Moritz Heiber)</author>
      <guid>http://localhost:1313/post/dynamics-dns-with-pfsense-and-route53/</guid>
      <description>&lt;p&gt;The quite excellent &lt;a href=&#34;https://pfsense.org&#34;&gt;pfSense&lt;/a&gt; comes with a &lt;a href=&#34;http://en.wikipedia.org/wiki/Dynamic_DNS&#34;&gt;dynamic DNS&lt;/a&gt; plugin for &lt;a href=&#34;http://aws.amazon.com/route53/&#34;&gt;Amazon&amp;rsquo;s Route53 DNS management service&lt;/a&gt;. However, there is little to no documentation provided on how to set it up properly and especially about setting up the &lt;a href=&#34;http://aws.amazon.com/iam/&#34;&gt;relevant IAM access policies&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So I went to the &lt;a href=&#34;https://github.com/pfsense/&#34;&gt;pfSense repository on GitHub&lt;/a&gt; and browsed the code in order to find out how much access the plugin needed in order to do its deed.&lt;/p&gt;

&lt;p&gt;As it turns out: not much. The following IAM policy will grant the plugin the required permissions to access Route53 on your behalf:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;Statement&amp;quot;:[
    {
      &amp;quot;Effect&amp;quot;:&amp;quot;Allow&amp;quot;,
      &amp;quot;Action&amp;quot;:[
        &amp;quot;route53:ChangeResourceRecordSets&amp;quot;,
        &amp;quot;route53:ListResourceRecordSets&amp;quot;
      ],
     &amp;quot;Resource&amp;quot;:&amp;quot;arn:aws:route53:::hostedzone/&amp;lt;your-hosted-zone-id&amp;gt;&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the &lt;code&gt;&amp;lt;your-hosted-zone-id&amp;gt;&lt;/code&gt; which you need to exchange for the ID your zone has been assigned to within Route53.&lt;/p&gt;

&lt;p&gt;What the dynamics DNS module within pfSense actually does:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Connect to the AWS API and look up all the records within the zone you configured&lt;/li&gt;
&lt;li&gt;Determines whether there is a record by the name you entered

&lt;ul&gt;
&lt;li&gt;If there is it deletes the record and adds it back with the new IP address attached to it.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Note: This is also why it is very wise to chose a very low (&amp;lt;= 60 seconds) TTL for your dynamic DNS record&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;If there isn&amp;rsquo;t it creates a new record and attaches the IP address to it&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Saves the IP address it just set within Route53 to a file within the pfSense environment to make sure it doesn&amp;rsquo;t update the same record twice

&lt;ul&gt;
&lt;li&gt;The IP either gets updated when the locally recorded IP address doesn&amp;rsquo;t match the record on Route53 or every 25 days&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After that the A record should get updated automatically each time the IP changes on the associated interface.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>